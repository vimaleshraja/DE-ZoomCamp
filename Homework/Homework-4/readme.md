# ğŸš• NYC Taxi Data Engineering Project (BigQuery + dbt)

This project builds an end-to-end data pipeline using:

* Python (data ingestion)
* Google Cloud Storage (raw storage)
* BigQuery (data warehouse)
* dbt (transformations: staging â†’ intermediate â†’ marts)

Follow the steps below to reproduce the full data model.

---

# ğŸ“Œ Prerequisites

* GCP Project with BigQuery enabled
* Service Account with:

  * BigQuery Data Editor
  * BigQuery Job User
  * Storage Admin
* Google Cloud Storage bucket
* Python 3.9+
* dbt-bigquery installed

---

# ğŸŸ¢ Step 1 â€” Upload Taxi Data to GCS

## 1ï¸âƒ£ Load Yellow & Green Taxi Data

File: `load_yellow_green.py`

This script:

* Downloads 2019â€“2020 Yellow and Green taxi data
* Converts data to Parquet
* Uploads files to your GCS bucket

### Update inside the file:

```python
BUCKET_NAME = "your-bucket-name"
PROJECT_ID = "your-project-id"
```

### Run:

```bash
python load_yellow_green.py
```

---

## 2ï¸âƒ£ Load FHV Taxi Data

File: `load_fhv_tripdata.py`

This script:

* Uploads FHV trip `csv.gz` files
* Stores them in your GCS bucket

### Update inside the file:

```python
BUCKET_NAME = "your-bucket-name"
PROJECT_ID = "your-project-id"
```

### Run:

```bash
python load_fhv_tripdata.py
```

After this step, all raw data should be available in your Cloud Storage bucket.

---

# ğŸ— Step 2 â€” Create BigQuery Tables

Open BigQuery Console and run:

```
create_green_yellow_dataset.sql
```

This script:

* Creates required datasets (if not existing)
* Creates tables from:

  * Parquet files (Yellow & Green)
  * CSV.GZ files (FHV)

After execution, verify:

* Yellow trip table
* Green trip table
* FHV trip table

---

# ğŸ”„ Step 3 â€” Run dbt Transformations

Navigate into the dbt folder:

```bash
cd dbt
```

Install dependencies:

```bash
dbt deps
```

Build full project:

```bash
dbt build --target prod
```

This will create:

* `dbt_prod_staging`
* `dbt_prod_intermediate`
* `dbt_prod_marts`

---

# ğŸŒ± Load Seed Data (If Needed)

```bash
dbt seed --target prod
```

---

# ğŸ§ª Run Tests

```bash
dbt test --target prod
```

---

# ğŸ“Š Generate Documentation

```bash
dbt docs generate
dbt docs serve
```

---

# ğŸ—‚ Data Model Layers

### Staging

* Cleaned raw source tables

### Intermediate

* Business logic transformations
* Derived fields and joins

### Marts

* Final fact and dimension tables
* Analytics-ready datasets

---

# âœ… Final Output

After completing all steps:

1. Data uploaded to GCS
2. BigQuery tables created
3. dbt models built

---

